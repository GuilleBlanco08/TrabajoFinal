{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDitZsQALa1H"
   },
   "source": [
    "# TRABAJO Parte 2: AIA_2024-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3yRPoVAo6vO"
   },
   "source": [
    "# Nombre y DNI del alumno/a 1:\n",
    "# Nombre y DNI del alumno/a 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C0VZXpaWwBZ"
   },
   "source": [
    "# Transfer Learning con CNNs - Dataset: Flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX-nPhwRLuBL"
   },
   "source": [
    "La idea de este trabajo es familiarizarnos con dos situaciones muy habituales en la actividad real de un \"Machine Learning Engineer\":\n",
    "\n",
    "1.   En primer lugar, con una de las técnicas más potentes asociadas con las redes neuronales: el **Transfer Learning**. Dado que las redes neuronales, para resolver un problema, capturan en su estructura de capas y pesos una representación jerárquica del problema.\n",
    "Entonces..., ¿por que no aprovechar ese conocimiento obtenido, para resolver otro problema diferente?\n",
    "\n",
    "2.  En segundo lugar, con la **busqueda de información sobre conceptos nuevos**. En este caso, los dos primeros modelos a implementar los hemos trabajado en clase. No así el Transfer Learning, y por tanto, debereis buscar vosotros mismos como hacer lo que se pide para el Modelo 3. Consultar en blogs, web y tutoriales es algo común en el día a día de alguien que quiere profundizar en el ML y, para ello, existen infinidad de fuentes. A modo de ejemplo, una fuente para profundizar en el Transfer Learning con redes convolucionales es: https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/\n",
    "\n",
    "En este trabajo vamos intentar resolver un problema de clasificación sobre un dataset propuesto por Tensorflow en 2019 conocido como \"flowers\". Este conjunto está formado por 3670 imágenes de flores pertenecientes a 5 clases diferentes. Para ello implementaremos 3 modelos:\n",
    "\n",
    "*   Modelo 1: implementación de una CNN básica.\n",
    "*   Modelo 2: es una evolución del modelo anterior, aplicando técnicas que reduzcan el overfitting.\n",
    "*   Modelo 3: rompemos la barrera de tener que seguir complicando nuestro modelo y se pide aplicar transfer learning utilizando un pre-trained model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PUYhaYNf9M0"
   },
   "source": [
    "# a) Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "B9DNPpbPPTFN",
    "outputId": "dc48f1e0-7756-44cf-a6d7-acb0cee6a1e2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWbySf7nLa1-"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLZg4N3QqKtJ"
   },
   "source": [
    "Descargamos el dataset que pone a nuestra disposición Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvfQsk5kGeXl",
    "outputId": "9b345432-df63-4d7e-929a-2a88b673e793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "\u001b[1m228813984/228813984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "zip_file = tf.keras.utils.get_file(origin=_URL,\n",
    "                                   fname=\"flower_photos.tgz\",\n",
    "                                   extract=True,\n",
    "                                   )\n",
    "\n",
    "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR636nTLHbAr"
   },
   "source": [
    "Tras completar la descarga, debemos tener la siguiente estructura de directorios:  \n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>~/.keras/datasets/flower_photos_extracted/flower_photos</b>\n",
    "|__ <b>daisy</b>\n",
    "|__ <b>dandelion</b>\n",
    "|__ <b>roses</b>\n",
    "|__ <b>sunflowers</b>\n",
    "|__ <b>tulips</b>\n",
    "</pre>\n",
    "\n",
    "Desgraciadamente, para este dataset, Tensorflow no nos proporciona la estructura de directorios necesaria de train y de validación. Por lo que debemos proceder del siguiente modo:\n",
    "\n",
    "* Crear una carpeta `train` y de `val`, cada una de ellas debe contener a su vez, cinco subdirectorios: uno para cada clase de flor.\n",
    "* Moveremos las imágenes de las carpetas originales a estas nuevas carpetas. De modo que el 80% de las imágenes vayan al conjunto de train y el 20% restante al de validación.\n",
    "* La estructura final de directorios debe ser la siguiente:\n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>~/.keras/datasets/flower_photos_extracted/flower_photos</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>daisy</b>: [12.jpg, 28.jpg, 31.jpg ....]\n",
    "    |______ <b>dandelion</b>: [41.jpg, 22.jpg, 35.jpg ....]\n",
    "    |______ <b>roses</b>: [121.jpg, 92.jpg, 38.jpg ....]\n",
    "    |______ <b>sunflowers</b>: [93.jpg, 23.jpg, 83.jpg ....]\n",
    "    |______ <b>tulips</b>: [109.jpg, 267.jpg, 93.jpg ....]\n",
    " |__ <b>val</b>\n",
    "    |______ <b>daisy</b>: [507.jpg, 508.jpg, 509.jpg ....]\n",
    "    |______ <b>dandelion</b>: [719.jpg, 720.jpg, 721.jpg ....]\n",
    "    |______ <b>roses</b>: [514.jpg, 515.jpg, 516.jpg ....]\n",
    "    |______ <b>sunflowers</b>: [560.jpg, 561.jpg, 562.jpg .....]\n",
    "    |______ <b>tulips</b>: [640.jpg, 641.jpg, 642.jpg ....]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTVvGj2tZnIH"
   },
   "source": [
    "Creamos una lista con el nombre de las 5 clases. En castellano sería: margaritas, diente de león, rosas, girasoles y tulipanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60aGWgMlZpTJ"
   },
   "outputs": [],
   "source": [
    "classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1PyDGL1aDCj"
   },
   "source": [
    "Creemos la estructura de directorios necesaria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4lKtmjIOo5Z"
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.expanduser(\"~/.keras/datasets/flower_photos_extracted/flower_photos/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htR3aJ2QXaxx"
   },
   "source": [
    "Preparamos variables con las rutas de los diferentes directorios a crear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V81auPngdF-"
   },
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "daisy_dir = os.path.join(train_dir, 'daisy')\n",
    "dandelion_dir = os.path.join(train_dir, 'dandelion')\n",
    "roses_dir = os.path.join(train_dir, 'roses')\n",
    "sunflowers_dir = os.path.join(train_dir, 'sunflowers')\n",
    "tulips_dir = os.path.join(train_dir, 'tulips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3R1AhKJdb87G",
    "outputId": "65d71ad9-b033-4060-960d-f6464dcf268f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roses: 641 Imagenes\n",
      "daisy: 633 Imagenes\n",
      "dandelion: 898 Imagenes\n",
      "sunflowers: 699 Imagenes\n",
      "tulips: 799 Imagenes\n"
     ]
    }
   ],
   "source": [
    "SPLIT_RATIO=0.8\n",
    "\n",
    "for cl in classes:\n",
    "    # path de las imagenes de la clase cl\n",
    "    img_path = os.path.join(base_dir, cl)\n",
    "\n",
    "    # obtenemos la lista de todas las imagenes\n",
    "    images = glob.glob(img_path + '/*.jpg')\n",
    "    print(\"{}: {} Imagenes\".format(cl, len(images)))\n",
    "\n",
    "    # determinamos cuantas imagenes son el 80%\n",
    "    num_train = int(round(len(images)*SPLIT_RATIO))\n",
    "\n",
    "    # separamos las imagenes en dos listas\n",
    "    train, val = images[:num_train], images[num_train:]\n",
    "\n",
    "    # creamos la carpeta de train/clase y val/clase\n",
    "    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
    "        os.makedirs(os.path.join(base_dir, 'train', cl))\n",
    "    else:\n",
    "        shutil.rmtree(os.path.join(base_dir, 'train', cl))\n",
    "\n",
    "    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n",
    "        os.makedirs(os.path.join(base_dir, 'val', cl))\n",
    "    else:\n",
    "        shutil.rmtree(os.path.join(base_dir, 'val', cl))\n",
    "\n",
    "    for t in train:\n",
    "        shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
    "\n",
    "    for v in val:\n",
    "        shutil.move(v, os.path.join(base_dir, 'val', cl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh6vmp4nQyxn"
   },
   "source": [
    "**Tarea 1: Muestre el nombre de dos ficheros cualquiera en alguna de esas rutas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgpsenXRNLzV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP0qLFsUwr3R"
   },
   "source": [
    "Es decir, la clase a la que pertenece cada imagen no viene dada por el nombre del fichero sino por el directorio en el que se encuentra almacenada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvdoqIPGxNO3"
   },
   "source": [
    "**Tarea 2: Muestra el número de imágenes de train que tenemos de cada clase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gk4dSZgNq6o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXiUqkuHx3j0"
   },
   "source": [
    "# b) Visualización del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMpndtLqLfN1"
   },
   "source": [
    "**Tarea 3: Muestra 3 imágenes de cada una de las clases, el título de la imagen será el shape del array de numpy asociado a la imagen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tbAfE_3GCLJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsjyjYWC0El4"
   },
   "source": [
    "# c) Modelo 1: CNN básica (objetivo: accuracy_valid > 60%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUThvU0o0Yan"
   },
   "source": [
    "Implemente una red convolucional para resolver el problema de clasificación. Para ello se sugiere una CNN con 3 capas convolucionales + pooling con la siguiente estructura:\n",
    "\n",
    "Bloque de procesamiento de imagen:\n",
    "1.   32 kernels -> 64 kernels -> 96 kernels\n",
    "2.   kernels de 3x3.\n",
    "3.   Stride = 1 y padding = SI.\n",
    "4.   Función de activación ReLU.\n",
    "5.   Maxpooling de 2x2 con stride clásico de 2 pixeles.\n",
    "6.   Igualamos el tamaño de todas las imágenes a 150 x 150.\n",
    "\n",
    "Bloque de decisión:\n",
    "7.   Capa densa de 512 neuronas.\n",
    "8.   Capa densa de salida.\n",
    "\n",
    "**Tarea 4: Define un modelo con la estructura anterior**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYLc-4hlWteF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V28tWVRj4XnU"
   },
   "source": [
    "**Tarea 5: Indica el shape de la imagen antes y después de cada capa de la red. Explica cómo has obtenido dichos valores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb6ZtlqM6tC6"
   },
   "source": [
    "|Capa| Shape a la salida| #parámetros |\n",
    "|:-|:-:|:-:|\n",
    "|Conv_1||\n",
    "|Pool_1||\n",
    "|Conv_2||\n",
    "|Pool_2||\n",
    "|flatten||\n",
    "|densa_1||\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZJMAJfN4zIR"
   },
   "source": [
    "**Tarea 6: Compara el resultado con un summary() del modelo**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XS4FCNLWWwFu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7XxJcrg4K5Q"
   },
   "source": [
    "**Tarea 7: Entrena el modelo de manera que obtenga un accuracy (sobre el conjunto de validación) > 60%.**\n",
    "\n",
    "* Utilice el optimizador que considere más adecuado.\n",
    "*   Recuerda que si no se realiza conversión a One-Hot de la etiqueta a predecir, debes utilizar como función de error `SparseCategoricalCrossentropy` (este es el procedimiento que hemos usado en clase).\n",
    "*   Considera un learning rate en el entorno de 0.001.\n",
    "*   En el caso de los generators utiliza `class_mode='sparse'`.\n",
    "*   Puedes utilizar p.e. un `batch_size = 100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ONBmzLkWwS-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPfDU2p0WfCw"
   },
   "source": [
    "**Tarea 8: Muestra la evolución de la función de error (train y valid) durante el entrenamiento.Explica que problema presenta el modelo que hemos entrenado.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwrinxkCYhxw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoYA2oVr4gkz"
   },
   "source": [
    "# d) Modelo 2: reducción del overfitting (objetivo: accuracy_valid > 70%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ok65EUWQCU_q"
   },
   "source": [
    "Para mejorar el accuracy del modelo, vamos a incorporar las dos técnicas más habituales de reducción del overfitting:\n",
    "\n",
    "    * drop-out\n",
    "    * data augmentation\n",
    "    \n",
    "\n",
    "**Tarea 9: Explica en que consisten y qué utilidad tienen para nuestro problema**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFw8epb88yTC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9amUdrG7x48"
   },
   "source": [
    "**Tarea 10: Construye un nuevo modelo 2 incorporando (en el modelo 1 anterior) el dropout adecuado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbT01-lpddH2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1Y9AJx6-ISZ"
   },
   "source": [
    "**Tarea 11: Explica que tipos de augmentation vas a considerar y que utilidad tienen en nuestro problema de clasificacion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaFE9z8T-Wqf"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip7dUHFK-a9c"
   },
   "source": [
    "**Tarea 12: Entrena el modelo de manera que obtenga un accuracy (sobre el conjunto de validación) > 70%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKgb3LZK-MG3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_eAQ33JCqcr"
   },
   "source": [
    "**Tarea 13: Muestra la evolucion de la funcion de error durante el entrenamiento. Explica qué diferencias de comportamiento hay entre las gráficas del modelo 1 y el modelo 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oMupPC2dT4f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLlScafUEH-J"
   },
   "source": [
    "# c) Modelo 3: Transfer Learning (objetivo: accuracy_valid > 88%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wct4sXqnGma1"
   },
   "source": [
    "En nuestro problema de clasificación de flores, el utilizar CNNs diseñadas y entrenadas por nosotros mismos, aparece una barrera en las proximidades del 75-77% de precisión del modelo (siempre sobre validación).\n",
    "\n",
    "Para superar este escollo, el siguiente paso natural es la utilización de modelos preentrenados. Existe una gran variedad de ellos basados en redes CNNs clásicas, donde la principal diferencia es que acumulan más capas que nuestros modelos 1 y 2. Adicionalmente, en estos modelos se han ido incorporando diferentes propuestas para mejorar la arquitectura de la CNN.\n",
    "\n",
    "En general, utilizando estos modelos convolucionales preentrenados podemos alcanzar accuracies próximos al 90%. Normalmente, estos modelos han sido previamente entrenados sobre datasets de gran tamaño y con gran número de categorías. P.e. en subconjuntos de Imagenet (14 millones de imagenes de 22K categorías).\n",
    "\n",
    "Dado que estos modelos se entrenaron para resolver un problema \"relativamente\" parecido a nuestro problema de clasificación, parece razonable pensar que podemos aprovechar ese conocimiento capturado en la red para resolver nuestro problema de clasificación de flores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEv-W62PE_Uz"
   },
   "source": [
    "Para elegir el modelo preentrenado que debe utilizar cada grupo, proceda del siguiente modo:\n",
    "* Paso 1: sume los DNIs de los componentes del grupo (si el grupo tiene un sólo miembro, vaya directamente al paso 2). res = dni_1 + dn_2\n",
    "* Paso 2: Aplique la siguiente operación al resultado anterior: res mod 6.\n",
    "* Paso 3: Tome el modelo cuyo número asociado coincide con el resultado de la operación anterior.\n",
    "* Paso 4: El porcentaje que aparece entre paréntesis junto al nombre del modelo es el accuracy (en validación) que deberías poder alcanzar sin dificultad utilizando el modelo. En todos los casos considera un input_shape = (224, 224, 3).\n",
    "\n",
    "  0. Resnet50 (>90%)\n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50\n",
    "\n",
    "  1. Resnet101 (>90%) input_shape = (224, 224, 3)\n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet101\n",
    "\n",
    "  2. VGG16 (>90%) input_shape = (224, 224, 3)\n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16\n",
    "\n",
    "  3. VGG19 (>90%) input_shape = (224, 224, 3) https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19/VGG19\n",
    "\n",
    "  4. Xception (>88%) input_shape = (224, 224, 3) https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception/Xception\n",
    "\n",
    "  5. Inceptionv3 (>88%) input_shape = (224, 224, 3) https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/InceptionV3\n",
    "\n",
    "P.e. si los DNIs de los alumnos son: 12345678 y 23456781. La suma es 35802459. De donde 35802459 mod 6 = 3. Por tanto, tomaríamos el modelo VGG16.\n",
    "\n",
    "Recuerda que puedes utilizar el siguiente post como referencia del uso de transfer learning: https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhVjwjvEG9Vp"
   },
   "source": [
    "**Tarea 14: Importa el modelo desde Tensorflow**\n",
    "\n",
    "Dado que este modelo ha sido entrenado para clasificar entre 1.000 categorías, las capas densas finales del modelo no son útiles para nuestro problema de clasificación de 5 categorías (es lo que suele llamarse include_top = SI/NO). De manera que eliminamos lo que a veces se suele llamar el \"top model\". De este modo sólo nos quedamos con la parte que hace la funcionalidad de \"procesamiento\" de la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DEx2Wt7EwMm"
   },
   "source": [
    "**Tarea 15: Personalizar el bloque de decisión**  \n",
    "Añadimos una capa de flatten y tres nuevas capas densas especificas para nuestro problema con dimensiones 4096, 1072 y la que necesite la capa de salida (con sus correspondientes drop-outs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQjE646eIIRL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P_FzD1YIMwx"
   },
   "source": [
    "**Tarea 16: Congelar los pesos que no se vayan a entrenar**\n",
    "\n",
    "Previo a hacer el denominado `Fine-Tuning` del modelo, indicaremos a Tensorflow que únicamente debe entrenar:\n",
    "\n",
    "   * Las dos últimas capas convolucionales de la red preentrenada, de las que realizaremos un ajuste fino de los pesos.\n",
    "   * Las tres capas densas que hemos incluido nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogbqIwMlJQ03"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWuP_cVFHIUA"
   },
   "source": [
    "**Tarea 17: Crear los datagenerators oportunos**\n",
    "\n",
    "Para ello:\n",
    "> * Utiliza Data augmentation.  \n",
    "> * Las imágenes tienen que ser preprocesadas igual que cuando se entrenó el modelo pre.entrenado original. Para ello se utiliza el parámetro `preprocessing_function=preprocess_input` (preprocess_input importado desde `keras.applications.xxxxxx` en ambos generators (train y valid). Por tanto, no hay que indicarle `rescale`. En caso de ser necesario, se encargará `preprocess_input`.  \n",
    "> * Dado que estamos reutilizando un modelo que no \"es nuestro\", deberemos ceñirnos al tamaño de imagen que permite la red a la entrada. Recuerda que debe ser: 224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQ96DlIbN3eh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai89loLzKiI7"
   },
   "source": [
    "**Tarea 18: Haz el fine-tuning del modelo con el objetivo de alcanzar un accuracy (sobre el conjunto de validación > 88%).**\n",
    "\n",
    "A la hora de entrenar un modelo pretrained es típico bajar el learning rate respecto al que utilizaríamos para un modelo nuestro desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWGH90NPR1gx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
